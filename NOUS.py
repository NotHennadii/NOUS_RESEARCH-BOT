import requests
import random
import time
import threading
import asyncio
import aiohttp
from itertools import cycle
from rich.text import Text
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn, MofNCompleteColumn
from rich.panel import Panel
from rich.align import Align
from rich.live import Live
from datetime import datetime, timedelta
import json
from dataclasses import dataclass
from typing import List, Optional, Tuple
import sys
from pathlib import Path

# Initialize Rich Console with enhanced features
console = Console(record=True, width=120)

# Configuration
API_URL = "https://inference-api.nousresearch.com/v1/chat/completions"

MODEL_WEIGHTS = {
    "Hermes-3-Llama-3.1-405B": 5,
    "Hermes-3-Llama-3.1-70B": 15,
    "DeepHermes-3-Mistral-24B-Preview": 30,
    "DeepHermes-3-Llama-3-8B-Preview": 50
}

MAX_RETRIES = 3
MIN_QUESTION_LENGTH = 30  # –£–º–µ–Ω—å—à–∏–ª –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
MAX_QUESTION_LENGTH = 40  # –£–º–µ–Ω—å—à–∏–ª —Å 60 –¥–æ 50 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤

# Global statistics
@dataclass
class Stats:
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_tokens: int = 0
    start_time: Optional[datetime] = None
    avg_response_time: float = 0.0
    total_input_tokens: int = 0
    total_output_tokens: int = 0

stats = Stats()
stats_lock = threading.Lock()

def print_modern_banner():
    """Original banner with token optimization notice"""
    banner_text = Text(
        """
*--------------------------------------------------------------------------*..=#####**=.  
*                            ...:-==++++==-:..                             *..=##=::+##=. 
*                       .:-:.....::-+*######***+-.                         *..=##*++*#*-. 
*                   .-+*********+:. ...=*########**=.                      *..=##*++=-..  
*                .=*###############*:   .-###########+.                    *..=##-.       
*            ..-*####################*:.  .+###########+..                 *..=**-.       
*          ..=#####################****=.. .-*#######**#:-.                *.   .....     
*        ..=*#####**###########++*+. .-++.  .-*##*##*+:=.==.               *...-**##*=:.  
*       .:=+*####++*##*+***###*+:-...-+*==   .==+:=+=.. -**=.              *..+#*=::*#*-. 
*      .:-:::+*++-:-==:::  --:.. .=**###**+   .+*==***++**#*-.             *.-##+.  -##+. 
*      :=:...::======++****+*+-==+*#######*:   :##**###**###*:             *.-##+.  -##+. 
*     .*+******#*##########################+.-=+*############*.            *..*#*-..*#*-. 
*     =#################**#################*-+#=*###**########-            *...=*####*:.  
*    .*#################*+*################***##++*##+*########.           *........      
*    :*##################=+*##########################*########-           *..+******+-.. 
*    :*##########*+*#####*-+##########################**#######*.          *..+##=--*##=. 
*    .*##########*=:=+=++=-::=======+**################=#######*:          *..+##===*#*-. 
*     :*######*==:   ..:=*#**+-..    .=################**######*=.         *..+##+*##+:.  
*      .+#***#*=+=:.   .--*##+-++..  .=######+===-+#####+*######+.         *..+##:.*#*=.  
*      ...=+-+#*=#+.   ..::=+-=-..   .=#################*+######*-         *..+##:.:##*:. 
*          ..:#*==:.    ........     :+##################**######=.        *....... ..... 
*            -#*:                   .-###################*+*#####*:        *..+********:  
*            +*+-.:..               .-####################**######=.       *..=++*##*++:  
*           .**-.::                 :+####################***#####*:.      *.   .+##-     
*           .**=.-:..              .:######################**######-.      *.   .+##-     
*           :*#*:-*++=-:           .-######*###############**##**##*.      *.   .+##-     
*           -*#*+:--....           .*#####*-###################**+##=      *.   .+##-     
*    ..    .+###*+:..              :######-:####################+-*#*.     *.   ..::.     
*  .-:.    .+#####+.               :#####*.:*###################+:=##-     *.   :+++-     
*  -+:     -*######+.              =#####+######################+.=*#*.::  *.  .+###*:    
* .=*:.   .+#########*+***###*+:.  +####**-.-###################+.=*##.:+-.*. .-**+##+.   
* .=**=:::+###**###############*-. -####*...=##################*-:*###..+=.*. :+#=.*#*-.  
* :=+*######*+-##################==#*#####*####################*:+####.:++.*..=#######+:  
* .=+=+*#*+=:=###**###############+::*#########################=*####*.-*+.*.:**=:::+##=. 
*  .=*+=---=*###*+*############*+:. ..=*#############################::**-.*.:-:.   .--:. 
*   ..=*#######+-+###########*=..     ..:+#############*############--**=. *. .-==:       
*      ..:+=::.:*#############*=.     .:+**++-=*#+-::::-=+--*#####*-+#*=.  *. .=##-       
*        .-****###++*###########=. .-**+:.:.=**:.       ..:+==###***#=..   *. .=##-       
*          .:=++-.:*###########*#***+:..-.:**-.             -*-*#*=..      *. .=##-       
*               .:+##########*+*+..  .::.+*+.                .*=+.         *. .=##-       
*...............-*#####**+=**+*-....:+=.:*-....................=:..........*. .=##-....   
----------------------------------------------------------------------------. .:------:.          

                         [NOUS RESEARCH FUCKER BOT v3.0]
                  Built for the bleak cold vacuum of rate limits 
""",
        style="bold cyan"
    )
    console.print(banner_text)

def choose_model():
    """Smart model selection with weighted random choice"""
    models = list(MODEL_WEIGHTS.keys())
    weights = list(MODEL_WEIGHTS.values())
    return random.choices(models, weights=weights, k=1)[0]

def load_config_file(file_path: str) -> List[str]:
    """Enhanced file loading with better error handling"""
    try:
        path = Path(file_path)
        if not path.exists():
            console.print(f"‚ö†Ô∏è  [yellow]File not found: {file_path}[/yellow]")
            return []
        
        with open(path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        
        console.print(f"‚úÖ [green]Loaded {len(lines)} entries from {file_path}[/green]")
        return lines
    except Exception as e:
        console.print(f"‚ùå [red]Error loading {file_path}: {e}[/red]")
        return []

def load_prompts_for_profiles(file_path: str, num_profiles: int) -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ"""
    prompts = load_config_file(file_path)
    
    if not prompts:
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω - –°–î–ï–õ–ê–ù–û –ë–û–õ–ï–ï –ö–û–ù–°–ê–ô–ó–ù–´–ú
        default_prompt = "–¢—ã –∫—Ä–∞—Ç–∫–∏–π AI. –û—Ç–≤–µ—á–∞–π –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–æ, –±—É–∫–≤–∞–ª—å–Ω–æ –≤ –ø–∞—Ä—É —Å–ª–æ–≤ " 
        return [default_prompt] * num_profiles
    
    # –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –º–µ–Ω—å—à–µ —á–µ–º –ø—Ä–æ—Ñ–∏–ª–µ–π, —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º
    if len(prompts) < num_profiles:
        extended_prompts = []
        for i in range(num_profiles):
            extended_prompts.append(prompts[i % len(prompts)])
        return extended_prompts
    
    # –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –±–æ–ª—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ num_profiles
    return prompts[:num_profiles]

def estimate_tokens(text: str) -> int:
    """–ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
    # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞: 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è –ª–∞—Ç–∏–Ω–∏—Ü—ã, 2-3 –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
    if any(ord(char) > 127 for char in text):  # –ï—Å—Ç—å –Ω–µ-ASCII —Å–∏–º–≤–æ–ª—ã
        return max(1, len(text) // 2)  # –î–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
    else:
        return max(1, len(text) // 4)  # –î–ª—è –ª–∞—Ç–∏–Ω–∏—Ü—ã

async def send_async_request(session: aiohttp.ClientSession, prompt: str, api_key: str, 
                           system_prompt: str, max_tokens: int, temperature: float = 0.7, 
                           proxy: Optional[str] = None) -> dict:
    """Async request sender with improved token control"""
    model_name = choose_model()
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # –û—Ü–µ–Ω–∫–∞ –≤—Ö–æ–¥—è—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
    input_tokens = estimate_tokens(system_prompt + prompt)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Ö–æ–¥—è—â–∏–µ —Ç–æ–∫–µ–Ω—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç —Ä–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç
    if input_tokens > 1000:  # –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
        # –û–±—Ä–µ–∑–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        max_system_length = 200
        if len(system_prompt) > max_system_length:
            system_prompt = system_prompt[:max_system_length] + "..."
            console.print(f"‚ö†Ô∏è  [yellow]System prompt truncated to save tokens[/yellow]")
    
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    proxy_url = proxy if proxy else None
    
    for attempt in range(MAX_RETRIES):
        start_time = time.time()
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            async with session.post(API_URL, headers=headers, json=payload, 
                                  proxy=proxy_url, timeout=timeout) as response:
                elapsed = time.time() - start_time
                
                if response.status == 200:
                    data = await response.json()
                    content = data["choices"][0]["message"]["content"].strip()
                    
                    # –ü–æ–¥—Å—á–µ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                    output_tokens = estimate_tokens(content)
                    total_tokens = input_tokens + output_tokens
                    
                    # Update stats
                    with stats_lock:
                        stats.successful_requests += 1
                        stats.total_requests += 1
                        stats.total_tokens += total_tokens
                        stats.total_input_tokens += input_tokens
                        stats.total_output_tokens += output_tokens
                        if stats.successful_requests > 0:
                            stats.avg_response_time = (stats.avg_response_time * (stats.successful_requests - 1) + elapsed) / stats.successful_requests
                    
                    return {
                        "success": True,
                        "content": content,
                        "model": model_name,
                        "elapsed": elapsed,
                        "prompt": prompt,
                        "total_tokens": total_tokens,
                        "input_tokens": input_tokens,
                        "output_tokens": output_tokens
                    }
                
                elif response.status == 429:
                    console.print(f"‚è≥ [yellow]Rate limit hit, waiting {10 * (attempt + 1)}s...[/yellow]")
                    await asyncio.sleep(10 * (attempt + 1))
                    continue
                else:
                    error_text = await response.text()
                    with stats_lock:
                        stats.failed_requests += 1
                        stats.total_requests += 1
                    return {
                        "success": False,
                        "error": f"HTTP {response.status}: {error_text}",
                        "elapsed": elapsed
                    }
                    
        except Exception as e:
            if attempt == MAX_RETRIES - 1:
                with stats_lock:
                    stats.failed_requests += 1
                    stats.total_requests += 1
                return {
                    "success": False,
                    "error": str(e),
                    "elapsed": time.time() - start_time
                }
            await asyncio.sleep(5 * (attempt + 1))

async def generate_question_async(session: aiohttp.ClientSession, api_key: str, 
                                system_prompt: str, max_tokens: int, temperature: float,
                                lang: str = 'r', proxy: Optional[str] = None) -> Optional[str]:
    """Async question generator with optimized prompts"""
    # –ë–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
    prompts = {
        'r': f"–ö–æ—Ä–æ—Ç–∫–∏–π –≤–æ–ø—Ä–æ—Å –æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö/–ò–ò ({MIN_QUESTION_LENGTH}-{MAX_QUESTION_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤):",
        'e': f"Short tech/AI question ({MIN_QUESTION_LENGTH}-{MAX_QUESTION_LENGTH} chars):",
        'b': random.choice([
            f"–ö–æ—Ä–æ—Ç–∫–∏–π –≤–æ–ø—Ä–æ—Å –æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö/–ò–ò ({MIN_QUESTION_LENGTH}-{MAX_QUESTION_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤):",
            f"Short tech/AI question ({MIN_QUESTION_LENGTH}-{MAX_QUESTION_LENGTH} chars):"
        ])
    }
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–∏–π –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
    question_max_tokens = min(max_tokens // 2, 32)
    
    result = await send_async_request(
        session, prompts.get(lang, prompts['r']), 
        api_key, system_prompt, question_max_tokens, temperature, proxy
    )
    
    if result["success"]:
        question = result["content"].split('\n')[0].strip()
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã
        if len(question) > MAX_QUESTION_LENGTH:
            question = question[:MAX_QUESTION_LENGTH]
        elif len(question) < MIN_QUESTION_LENGTH:
            # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –¥–æ–ø–æ–ª–Ω—è–µ–º
            question = question + "?"
        return question
    return None

def format_proxies(raw_proxies: List[str]) -> List[str]:
    """Enhanced proxy formatter"""
    formatted = []
    for line in raw_proxies:
        line = line.strip()
        if not line:
            continue
            
        if "@" not in line and line.count(":") == 3:
            try:
                login, password, ip, port = line.split(":")
                formatted.append(f"http://{login}:{password}@{ip}:{port}")
            except ValueError:
                console.print(f"‚ö†Ô∏è  [yellow]Invalid proxy format: {line}[/yellow]")
        else:
            formatted.append(line if line.startswith(("http://", "https://", "socks5://")) else f"http://{line}")
    
    return formatted


async def worker_async(worker_id: int, count: int, delay_range: Tuple[float, float], 
                      api_keys: cycle, proxy: Optional[str], system_prompt: str, 
                      lang: str, max_tokens: int, temperature: float, progress: Progress, task_id: int):
    """Enhanced async worker with precise token control"""
    
    async with aiohttp.ClientSession() as session:
        for i in range(count):
            try:
                api_key = next(api_keys)
                
                # Generate question
                question = await generate_question_async(
                    session, api_key, system_prompt, max_tokens, temperature, lang, proxy
                )
                
                if question:
                    # Send main request with remaining tokens
                    remaining_tokens = max_tokens - 10  # –†–µ–∑–µ—Ä–≤ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                    result = await send_async_request(
                        session, question, api_key, system_prompt, remaining_tokens, temperature, proxy
                    )
                    
                    if result["success"]:
                        # Log success with detailed token info
                        timestamp = datetime.now().strftime('%H:%M:%S')
                        console.print(f"\n[{timestamp}] ‚úÖ [green]Worker-{worker_id}[/green] | [blue]{result['model']}[/blue] | {result['elapsed']:.2f}s")
                        console.print(f"üî§ [cyan]Tokens: {result['total_tokens']} (in:{result['input_tokens']} + out:{result['output_tokens']})[/cyan]")
                        console.print(f"[yellow]Q:[/yellow] {question}")
                        console.print(f"[green]A:[/green] {result['content'][:150]}{'...' if len(result['content']) > 150 else ''}")
                        
                        # Save to log with detailed token tracking
                        try:
                            with open("ai_stress_log.txt", "a", encoding="utf-8") as f:
                                f.write(f"\n[{datetime.now().isoformat()}] Worker-{worker_id} | {result['model']} | {result['elapsed']:.2f}s\n")
                                f.write(f"Tokens: {result['total_tokens']} (Input: {result['input_tokens']}, Output: {result['output_tokens']})\n")
                                f.write(f"Temperature: {temperature} | Max tokens: {max_tokens}\n")
                                f.write(f"Question: {question}\n")
                                f.write(f"Answer: {result['content']}\n")
                                f.write("="*80 + "\n")
                        except Exception as log_error:
                            console.print(f"‚ö†Ô∏è  [yellow]Log error: {log_error}[/yellow]")
                    else:
                        console.print(f"‚ùå [red]Worker-{worker_id} failed:[/red] {result['error']}")
                else:
                    console.print(f"‚ö†Ô∏è  [yellow]Worker-{worker_id}: Failed to generate question[/yellow]")
                
                progress.update(task_id, advance=1)
                
                if i < count - 1:
                    sleep_time = random.uniform(delay_range[0], delay_range[1])
                    await asyncio.sleep(max(0.1, sleep_time))
                    
            except Exception as worker_error:
                console.print(f"‚ùå [red]Worker-{worker_id} error: {worker_error}[/red]")
                progress.update(task_id, advance=1)

def get_enhanced_user_inputs():
    """Modern interactive input system with token and temperature control"""
    console.print("\nüéõÔ∏è[bold bright_cyan] –ú–ï–ù–Æ –ù–ê–°–¢–†–û–ï–ö (TOKEN CONTROL) [/bold bright_cyan]")
    
    try:
        # Requests per thread
        count_input = console.input("[bold]üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ AI [dim](default: 5)[/dim]: [/bold]")
        count = int(count_input) if count_input.strip() else 5
        
        # Token limit - strict control - –ò–∑–º–µ–Ω–∏–ª —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
        tokens_input = console.input("[bold]üî§ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ [dim](default: 80, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 50-80 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏)[/dim]: [/bold]")
        max_tokens = int(tokens_input) if tokens_input.strip() else 80
        
        # Temperature control
        temp_input = console.input("[bold]üå°Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å) [dim](0.1-2.0, default: 0.7)[/dim]: [/bold]")
        temperature = float(temp_input) if temp_input.strip() else 0.7
        temperature = max(0.1, min(2.0, temperature))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
        
        # Delay configuration
        delay_input = console.input("[bold]‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ [dim](e.g., '1 3' for random between 1-3, default: 2)[/dim]: [/bold]")
        if not delay_input.strip():
            delay_input = "2"
        delay_parts = delay_input.strip().split()
        
        if len(delay_parts) == 2:
            delay_min, delay_max = float(delay_parts[0]), float(delay_parts[1])
            if delay_min > delay_max:
                delay_min, delay_max = delay_max, delay_min
        else:
            delay_min = delay_max = float(delay_parts[0])
        
        # Thread count
        threads_input = console.input("[bold]üßµ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ [dim](default: 1)[/dim]: [/bold]")
        threads = int(threads_input) if threads_input.strip() else 1
        
        # Language selection
        lang_options = {"r": "Russian", "e": "English", "b": "Both"}
        console.print("\n[bold]üåç –í—ã–±–µ—Ä–∏ —è–∑—ã–∫:[/bold]")
        for key, value in lang_options.items():
            console.print(f"  [cyan]{key}[/cyan] - {value}")
        
        lang_input = console.input("[bold]–í—ã–±–µ—Ä–∏ —è–∑—ã–∫ [dim](default: b)[/dim]: [/bold]")
        lang_choice = lang_input.strip().lower() if lang_input.strip() else 'b'
        
        # Proxy configuration
        proxy_choices = []
        for i in range(threads):
            proxy_input = console.input(f"[bold]üåê –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏ –∫ –ø—Ä–æ—Ñ–∏–ª—é {i+1}? [dim](y/n, default: n)[/dim]: [/bold]")
            use_proxy = proxy_input.strip().lower() == 'y'
            proxy_choices.append(use_proxy)
        
        return count, (delay_min, delay_max), threads, lang_choice, proxy_choices, max_tokens, temperature
        
    except (ValueError, KeyboardInterrupt):
        console.print("‚ùå [red]Invalid input or interrupted. Please try again.[/red]")
        return get_enhanced_user_inputs()

async def main():
    """Enhanced main function with precise token control"""
    print_modern_banner()
    
    # Get user configuration
    count, delay_range, threads, lang_choice, proxy_choices, max_tokens, temperature = get_enhanced_user_inputs()
    
    # Display configuration
    config_table = Table(title="‚öôÔ∏è –°—É–º–º–∞—Ä–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞", border_style="bright_cyan")
    config_table.add_column("–ù–∞—Å—Ç—Ä–æ–π–∫–∞", style="bright_magenta")
    config_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="bright_cyan")
    
    config_table.add_row("–ö–æ–ª-–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤", str(count))
    config_table.add_row("–ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏", f"{delay_range[0]:.1f}s - {delay_range[1]:.1f}s")
    config_table.add_row("üî§ –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤", str(max_tokens))
    config_table.add_row("üå°Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", str(temperature))
    config_table.add_row("–ö–æ–ª-–≤–æ –ø–æ—Ç–æ–∫–æ–≤", str(threads))
    config_table.add_row("–Ø–∑—ã–∫", {"r": "Russian", "e": "English", "b": "Both"}[lang_choice])
    config_table.add_row("–ü—Ä–æ–∫—Å–∏", str(sum(proxy_choices)))
    
    console.print("\n")
    console.print(config_table)
    
    # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
    estimated_total_tokens = count * threads * max_tokens * 2  # x2 –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤
    console.print(f"üí∞ [yellow]–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤: ~{estimated_total_tokens:,}[/yellow]")
    
    console.input("\n[bold bright_yellow]–ñ–º–µ–º Enter –¥–ª—è –Ω–∞—á–∞–ª–∞...[/bold bright_yellow]")
    
    # Load configuration files
    api_keys = load_config_file("API_keys.txt")
    if not api_keys:
        console.print("‚ùå [red]No API keys found. Please add keys to API_keys.txt[/red]")
        console.print("üí° [blue]Create API_keys.txt file and add your API keys (one per line)[/blue]")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
    system_prompts = load_prompts_for_profiles("promt.txt", threads)
    
    raw_proxies = load_config_file("proxy.txt")
    formatted_proxies = format_proxies(raw_proxies)
    proxy_pool = cycle(formatted_proxies) if formatted_proxies else cycle([None])
    
    keys_cycle = cycle(api_keys)
    
    # Initialize statistics
    stats.start_time = datetime.now()
    
    # Create progress tracking
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        MofNCompleteColumn(),
        TextColumn("‚Ä¢"),
        TimeRemainingColumn(),
        console=console,
        transient=False,
    ) as progress:
        
        # Create tasks for each worker
        tasks = []
        worker_tasks = []
        
        for i in range(threads):
            proxy = next(proxy_pool) if proxy_choices[i] else None
            system_prompt = system_prompts[i]  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
            task_id = progress.add_task(f"Worker-{i+1}", total=count)
            tasks.append(task_id)
            
            worker_task = worker_async(
                i+1, count, delay_range, keys_cycle, proxy, 
                system_prompt, lang_choice, max_tokens, temperature, progress, task_id
            )
            worker_tasks.append(worker_task)
        
        # Run all workers concurrently
        console.print(f"\nüöÄ [bold bright_green]Starting {threads} workers with {count} requests each...[/bold bright_green]")
        console.print(f"üî§ [cyan]Token limit per response: {max_tokens} | Temperature: {temperature}[/cyan]\n")
        
        # Create live stats display in separate task
        stats_task = asyncio.create_task(update_stats_display())
        
        try:
            await asyncio.gather(*worker_tasks)
        except KeyboardInterrupt:
            console.print("\n‚ö†Ô∏è  [yellow]Interrupted by user[/yellow]")
        finally:
            stats_task.cancel()
    
    # Final summary
    console.print("\n" + "="*80)
    console.print(create_stats_table())
    
    # Token usage summary
    with stats_lock:
        console.print(f"\nüí∞ [bold bright_green]–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–û–ö–ï–ù–û–í:[/bold bright_green]")
        console.print(f"üî§ –í—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: [bright_cyan]{stats.total_tokens:,}[/bright_cyan]")
        console.print(f"üì• –í—Ö–æ–¥—è—â–∏–µ —Ç–æ–∫–µ–Ω—ã: [bright_yellow]{stats.total_input_tokens:,}[/bright_yellow]")
        console.print(f"üì§ –ò—Å—Ö–æ–¥—è—â–∏–µ —Ç–æ–∫–µ–Ω—ã: [bright_green]{stats.total_output_tokens:,}[/bright_green]")
        
        if stats.successful_requests > 0:
            avg_total = stats.total_tokens / stats.successful_requests
            avg_input = stats.total_input_tokens / stats.successful_requests
            avg_output = stats.total_output_tokens / stats.successful_requests
            console.print(f"üìä –°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –∑–∞–ø—Ä–æ—Å: [bright_magenta]{avg_total:.1f}[/bright_magenta] —Ç–æ–∫–µ–Ω–æ–≤ ({avg_input:.1f} in + {avg_output:.1f} out)")
    
    console.print("\n‚úÖ [bold bright_green]Stress test completed![/bold bright_green]")
    console.print(f"üìÑ [cyan]Detailed logs saved to: ai_stress_log.txt[/cyan]")

async def update_stats_display():
    """Update stats display periodically"""
    while True:
        await asyncio.sleep(5)
        # Print current stats every 5 seconds
        console.print("\n" + "‚îÄ" * 40)
        console.print(create_stats_table())

if __name__ == "__main__":
    try:
        # Check Python version
        if sys.version_info < (3, 7):
            console.print("‚ùå [red]Python 3.7 or higher is required[/red]")
            sys.exit(1)
            
        asyncio.run(main())
    except KeyboardInterrupt:
        console.print("\nüëã [yellow]Goodbye![/yellow]")
    except Exception as e:
        console.print(f"\nüí• [red]Unexpected error: {e}[/red]")
        console.print_exception()