import requests
import random
import time
import threading
import asyncio
import aiohttp
from itertools import cycle
from rich.text import Text
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn, MofNCompleteColumn
from rich.panel import Panel
from rich.align import Align
from rich.live import Live
from datetime import datetime, timedelta
import json
from dataclasses import dataclass
from typing import List, Optional, Tuple
import sys
import gc
import weakref
from pathlib import Path

# Initialize Rich Console –ë–ï–ó record=True –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
console = Console(width=120)  # –£–±—Ä–∞–ª record=True

# Configuration
API_URL = "https://inference-api.nousresearch.com/v1/chat/completions"

MODEL_WEIGHTS = {
    "Hermes-3-Llama-3.1-405B": 5,
    "Hermes-3-Llama-3.1-70B": 15,
    "DeepHermes-3-Mistral-24B-Preview": 30,
    "DeepHermes-3-Llama-3-8B-Preview": 50
}

MAX_RETRIES = 3
MIN_QUESTION_LENGTH = 30
MAX_QUESTION_LENGTH = 40

# Global statistics —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞
@dataclass
class Stats:
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_tokens: int = 0
    start_time: Optional[datetime] = None
    avg_response_time: float = 0.0
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    # –î–æ–±–∞–≤–ª—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—é
    _recent_responses: List = None
    
    def __post_init__(self):
        self._recent_responses = []
    
    def add_response_time(self, elapsed: float):
        """–î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∏—Å—Ç–æ—Ä–∏–∏"""
        self._recent_responses.append(elapsed)
        # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π
        if len(self._recent_responses) > 100:
            self._recent_responses = self._recent_responses[-50:]  # –û—Å—Ç–∞–≤–ª—è–µ–º 50
        
        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è
        if self._recent_responses:
            self.avg_response_time = sum(self._recent_responses) / len(self._recent_responses)

stats = Stats()
stats_lock = threading.Lock()

def print_modern_banner():
    """Original banner with token optimization notice"""
    banner_text = Text(
        """
*--------------------------------------------------------------------------*..=#####**=.  
*                            ...:-==++++==-:..                             *..=##=::+##=. 
*                       .:-:.....::-+*######***+-.                         *..=##*++*#*-. 
*                   .-+*********+:. ...=*########**=.                      *..=##*++=-..  
*                .=*###############*:   .-###########+.                    *..=##-.       
*            ..-*####################*:.  .+###########+..                 *..=**-.       
*          ..=#####################****=.. .-*#######**#:-.                *.   .....     
*        ..=*#####**###########++*+. .-++.  .-*##*##*+:=.==.               *...-**##*=:.  
*       .:=+*####++*##*+***###*+:-...-+*==   .==+:=+=.. -**=.              *..+#*=::*#*-. 
*      .:-:::+*++-:-==:::  --:.. .=**###**+   .+*==***++**#*-.             *.-##+.  -##+. 
*      :=:...::======++****+*+-==+*#######*:   :##**###**###*:             *.-##+.  -##+. 
*     .*+******#*##########################+.-=+*############*.            *..*#*-..*#*-. 
*     =#################**#################*-+#=*###**########-            *...=*####*:.  
*    .*#################*+*################***##++*##+*########.           *........      
*    :*##################=+*##########################*########-           *..+******+-.. 
*    :*##########*+*#####*-+##########################**#######*.          *..+##=--*##=. 
*    .*##########*=:=+=++=-::=======+**################=#######*:          *..+##===*#*-. 
*     :*######*==:   ..:=*#**+-..    .=################**######*=.         *..+##+*##+:.  
*      .+#***#*=+=:.   .--*##+-++..  .=######+===-+#####+*######+.         *..+##:.*#*=.  
*      ...=+-+#*=#+.   ..::=+-=-..   .=#################*+######*-         *..+##:.:##*:. 
*          ..:#*==:.    ........     :+##################**######=.        *....... ..... 
*            -#*:                   .-###################*+*#####*:        *..+********:  
*            +*+-.:..               .-####################**######=.       *..=++*##*++:  
*           .**-.::                 :+####################***#####*:.      *.   .+##-     
*           .**=.-:..              .:######################**######-.      *.   .+##-     
*           :*#*:-*++=-:           .-######*###############**##**##*.      *.   .+##-     
*           -*#*+:--....           .*#####*-###################**+##=      *.   .+##-     
*    ..    .+###*+:..              :######-:####################+-*#*.     *.   ..::.     
*  .-:.    .+#####+.               :#####*.:*###################+:=##-     *.   :+++-     
*  -+:     -*######+.              =#####+######################+.=*#*.::  *.  .+###*:    
* .=*:.   .+#########*+***###*+:.  +####**-.-###################+.=*##.:+-.*. .-**+##+.   
* .=**=:::+###**###############*-. -####*...=##################*-:*###..+=.*. :+#=.*#*-.  
* :=+*######*+-##################==#*#####*####################*:+####.:++.*..=#######+:  
* .=+=+*#*+=:=###**###############+::*#########################=*####*.-*+.*.:**=:::+##=. 
*  .=*+=---=*###*+*############*+:. ..=*#############################::**-.*.:-:.   .--:. 
*   ..=*#######+-+###########*=..     ..:+#############*############--**=. *. .-==:       
*      ..:+=::.:*#############*=.     .:+**++-=*#+-::::-=+--*#####*-+#*=.  *. .=##-       
*        .-****###++*###########=. .-**+:.:.=**:.       ..:+==###***#=..   *. .=##-       
*          .:=++-.:*###########*#***+:..-.:**-.             -*-*#*=..      *. .=##-       
*               .:+##########*+*+..  .::.+*+.                .*=+.         *. .=##-       
*...............-*#####**+=**+*-....:+=.:*-....................=:..........*. .=##-....   
----------------------------------------------------------------------------. .:------:.          

                         [NOUS RESEARCH FUCKER BOT v3.3]
                  Built for the bleak cold vacuum of rate limits 
""",
        style="bold cyan"
    )
    console.print(banner_text)


def choose_model():
    """Smart model selection with weighted random choice"""
    models = list(MODEL_WEIGHTS.keys())
    weights = list(MODEL_WEIGHTS.values())
    return random.choices(models, weights=weights, k=1)[0]

def load_config_file(file_path: str) -> List[str]:
    """Enhanced file loading with better error handling"""
    try:
        path = Path(file_path)
        if not path.exists():
            console.print(f"‚ö†Ô∏è  [yellow]File not found: {file_path}[/yellow]")
            return []
        
        with open(path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        
        console.print(f"‚úÖ [green]Loaded {len(lines)} entries from {file_path}[/green]")
        return lines
    except Exception as e:
        console.print(f"‚ùå [red]Error loading {file_path}: {e}[/red]")
        return []

def load_prompts_for_profiles(file_path: str, num_profiles: int) -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ"""
    prompts = load_config_file(file_path)
    
    if not prompts:
        default_prompt = "–¢—ã –∫—Ä–∞—Ç–∫–∏–π AI. –û—Ç–≤–µ—á–∞–π –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–æ, –±—É–∫–≤–∞–ª—å–Ω–æ –≤ –ø–∞—Ä—É —Å–ª–æ–≤ " 
        return [default_prompt] * num_profiles
    
    if len(prompts) < num_profiles:
        extended_prompts = []
        for i in range(num_profiles):
            extended_prompts.append(prompts[i % len(prompts)])
        return extended_prompts
    
    return prompts[:num_profiles]

def estimate_tokens(text: str) -> int:
    """–ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
    if any(ord(char) > 127 for char in text):
        return max(1, len(text) // 2)
    else:
        return max(1, len(text) // 4)

async def send_async_request(session: aiohttp.ClientSession, prompt: str, api_key: str, 
                           system_prompt: str, max_tokens: int, temperature: float = 0.7, 
                           proxy: Optional[str] = None) -> dict:
    """Async request sender with memory optimization"""
    model_name = choose_model()
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    input_tokens = estimate_tokens(system_prompt + prompt)
    
    if input_tokens > 1000:
        max_system_length = 200
        if len(system_prompt) > max_system_length:
            system_prompt = system_prompt[:max_system_length] + "..."
    
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    for attempt in range(MAX_RETRIES):
        start_time = time.time()
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏
            connector_kwargs = {}
            if proxy:
                connector_kwargs['proxy'] = proxy
                
            async with session.post(API_URL, headers=headers, json=payload, 
                                  timeout=timeout, **connector_kwargs) as response:
                elapsed = time.time() - start_time
                
                if response.status == 200:
                    data = await response.json()
                    content = data["choices"][0]["message"]["content"].strip()
                    
                    output_tokens = estimate_tokens(content)
                    total_tokens = input_tokens + output_tokens
                    
                    # Update stats —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
                    with stats_lock:
                        stats.successful_requests += 1
                        stats.total_requests += 1
                        stats.total_tokens += total_tokens
                        stats.total_input_tokens += input_tokens
                        stats.total_output_tokens += output_tokens
                        stats.add_response_time(elapsed)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥
                    
                    # –û—á–∏—â–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
                    del data
                    
                    return {
                        "success": True,
                        "content": content,
                        "model": model_name,
                        "elapsed": elapsed,
                        "prompt": prompt[:100],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                        "total_tokens": total_tokens,
                        "input_tokens": input_tokens,
                        "output_tokens": output_tokens
                    }
                
                elif response.status == 429:
                    await asyncio.sleep(10 * (attempt + 1))
                    continue
                else:
                    error_text = await response.text()
                    with stats_lock:
                        stats.failed_requests += 1
                        stats.total_requests += 1
                    return {
                        "success": False,
                        "error": f"HTTP {response.status}",  # –°–æ–∫—Ä–∞—â–∞–µ–º error –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                        "elapsed": elapsed
                    }
                    
        except Exception as e:
            if attempt == MAX_RETRIES - 1:
                with stats_lock:
                    stats.failed_requests += 1
                    stats.total_requests += 1
                return {
                    "success": False,
                    "error": str(e)[:100],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –æ—à–∏–±–∫–∏
                    "elapsed": time.time() - start_time
                }
            await asyncio.sleep(5 * (attempt + 1))

async def generate_question_async(session: aiohttp.ClientSession, api_key: str, 
                                system_prompt: str, max_tokens: int, temperature: float,
                                lang: str = 'r', proxy: Optional[str] = None) -> Optional[str]:
    """Async question generator with memory optimization"""
    prompts = {
        'r': f"–ö–æ—Ä–æ—Ç–∫–∏–π –≤–æ–ø—Ä–æ—Å –æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö/–ò–ò ({MIN_QUESTION_LENGTH}-{MAX_QUESTION_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤):",
        'e': f"Short tech/AI question ({MIN_QUESTION_LENGTH}-{MAX_QUESTION_LENGTH} chars):",
        'b': random.choice([
            f"–ö–æ—Ä–æ—Ç–∫–∏–π –≤–æ–ø—Ä–æ—Å –æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö/–ò–ò ({MIN_QUESTION_LENGTH}-{MAX_QUESTION_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤):",
            f"Short tech/AI question ({MIN_QUESTION_LENGTH}-{MAX_QUESTION_LENGTH} chars):"
        ])
    }
    
    question_max_tokens = min(max_tokens // 2, 32)
    
    result = await send_async_request(
        session, prompts.get(lang, prompts['r']), 
        api_key, system_prompt, question_max_tokens, temperature, proxy
    )
    
    if result["success"]:
        question = result["content"].split('\n')[0].strip()
        if len(question) > MAX_QUESTION_LENGTH:
            question = question[:MAX_QUESTION_LENGTH]
        elif len(question) < MIN_QUESTION_LENGTH:
            question = question + "?"
        
        # –û—á–∏—â–∞–µ–º result –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
        del result
        return question
    
    del result
    return None

def format_proxies(raw_proxies: List[str]) -> List[str]:
    """Enhanced proxy formatter"""
    formatted = []
    for line in raw_proxies:
        line = line.strip()
        if not line:
            continue
            
        if "@" not in line and line.count(":") == 3:
            try:
                login, password, ip, port = line.split(":")
                formatted.append(f"http://{login}:{password}@{ip}:{port}")
            except ValueError:
                console.print(f"‚ö†Ô∏è  [yellow]Invalid proxy format: {line}[/yellow]")
        else:
            formatted.append(line if line.startswith(("http://", "https://", "socks5://")) else f"http://{line}")
    
    return formatted

async def worker_async(worker_id: int, count: int, delay_range: Tuple[float, float], 
                      api_keys: cycle, proxy: Optional[str], system_prompt: str, 
                      lang: str, max_tokens: int, temperature: float, progress: Progress, task_id: int):
    """Enhanced async worker with memory optimization"""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
    connector = aiohttp.TCPConnector(
        limit=10,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        limit_per_host=5,
        ttl_dns_cache=300,
        use_dns_cache=True,
        keepalive_timeout=30,
        enable_cleanup_closed=True  # –í–∞–∂–Ω–æ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∑–∞–∫—Ä—ã—Ç—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
    )
    
    timeout = aiohttp.ClientTimeout(total=60, connect=30)
    
    async with aiohttp.ClientSession(
        connector=connector, 
        timeout=timeout,
        headers={'Connection': 'close'}  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
    ) as session:
        
        for i in range(count):
            try:
                api_key = next(api_keys)
                
                # Generate question
                question = await generate_question_async(
                    session, api_key, system_prompt, max_tokens, temperature, lang, proxy
                )
                
                if question:
                    remaining_tokens = max_tokens - 10
                    result = await send_async_request(
                        session, question, api_key, system_prompt, remaining_tokens, temperature, proxy
                    )
                    
                    if result["success"]:
                        timestamp = datetime.now().strftime('%H:%M:%S')
                        
                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                        console.print(f"[{timestamp}] ‚úÖ Worker-{worker_id} | {result['model']} | {result['elapsed']:.2f}s")
                        console.print(f"üî§ Tokens: {result['total_tokens']}")
                        
                        # –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –≤—ã–≤–æ–¥
                        short_question = question[:50] + "..." if len(question) > 50 else question
                        short_answer = result['content'][:100] + "..." if len(result['content']) > 100 else result['content']
                        
                        console.print(f"Q: {short_question}")
                        console.print(f"A: {short_answer}")
                        
                        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–æ—Ç–∞—Ü–∏–µ–π
                        await log_result_async(worker_id, result, question, temperature, max_tokens)
                        
                    else:
                        console.print(f"‚ùå Worker-{worker_id} failed: {result.get('error', 'Unknown error')}")
                    
                    # –û—á–∏—â–∞–µ–º result
                    del result
                else:
                    console.print(f"‚ö†Ô∏è Worker-{worker_id}: Failed to generate question")
                
                progress.update(task_id, advance=1)
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 10 –∑–∞–ø—Ä–æ—Å–æ–≤
                if i % 10 == 0:
                    gc.collect()
                
                if i < count - 1:
                    sleep_time = random.uniform(delay_range[0], delay_range[1])
                    await asyncio.sleep(max(0.1, sleep_time))
                    
            except Exception as worker_error:
                console.print(f"‚ùå Worker-{worker_id} error: {str(worker_error)[:100]}")
                progress.update(task_id, advance=1)

async def log_result_async(worker_id: int, result: dict, question: str, temperature: float, max_tokens: int):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–æ—Ç–∞—Ü–∏–µ–π —Ñ–∞–π–ª–æ–≤"""
    try:
        log_file = Path("ai_stress_log.txt")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –∏ —Ä–æ—Ç–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if log_file.exists() and log_file.stat().st_size > 50 * 1024 * 1024:  # 50MB
            backup_file = Path(f"ai_stress_log_backup_{int(time.time())}.txt")
            log_file.rename(backup_file)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        log_entry = (
            f"[{datetime.now().isoformat()}] W{worker_id} | {result['model']} | "
            f"{result['elapsed']:.2f}s | T:{result['total_tokens']}\n"
            f"Q: {question[:100]}\n"
            f"A: {result['content'][:200]}\n"
            f"{'='*40}\n"
        )
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(log_entry)
            
    except Exception as log_error:
        console.print(f"‚ö†Ô∏è Log error: {str(log_error)[:50]}")

def create_stats_table():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏"""
    table = Table(title="üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", border_style="bright_green")
    table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="bright_magenta")
    table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="bright_cyan")
    
    with stats_lock:
        success_rate = (stats.successful_requests / max(stats.total_requests, 1)) * 100
        
        table.add_row("–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤", str(stats.total_requests))
        table.add_row("–£—Å–ø–µ—à–Ω—ã—Ö", str(stats.successful_requests))
        table.add_row("–ù–µ—É–¥–∞—á–Ω—ã—Ö", str(stats.failed_requests))
        table.add_row("–£—Å–ø–µ—à–Ω–æ—Å—Ç—å", f"{success_rate:.1f}%")
        table.add_row("–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è", f"{stats.avg_response_time:.2f}s")
        table.add_row("–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤", f"{stats.total_tokens:,}")
    
    return table

def get_enhanced_user_inputs():
    """Modern interactive input system with token and temperature control"""
    console.print("\nüéõÔ∏è[bold bright_cyan] –ú–ï–ù–Æ –ù–ê–°–¢–†–û–ï–ö (TOKEN CONTROL) [/bold bright_cyan]")
    
    try:
        count_input = console.input("[bold]üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ AI [dim](default: 5)[/dim]: [/bold]")
        count = int(count_input) if count_input.strip() else 5
        
        tokens_input = console.input("[bold]üî§ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ [dim](default: 80, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 50-80 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏)[/dim]: [/bold]")
        max_tokens = int(tokens_input) if tokens_input.strip() else 80
        
        temp_input = console.input("[bold]üå°Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å) [dim](0.1-2.0, default: 0.7)[/dim]: [/bold]")
        temperature = float(temp_input) if temp_input.strip() else 0.7
        temperature = max(0.1, min(2.0, temperature))
        
        delay_input = console.input("[bold]‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ [dim](e.g., '1 3' for random between 1-3, default: 2)[/dim]: [/bold]")
        if not delay_input.strip():
            delay_input = "2"
        delay_parts = delay_input.strip().split()
        
        if len(delay_parts) == 2:
            delay_min, delay_max = float(delay_parts[0]), float(delay_parts[1])
            if delay_min > delay_max:
                delay_min, delay_max = delay_max, delay_min
        else:
            delay_min = delay_max = float(delay_parts[0])
        
        threads_input = console.input("[bold]üßµ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ [dim](default: 1)[/dim]: [/bold]")
        threads = int(threads_input) if threads_input.strip() else 1
        
        lang_options = {"r": "Russian", "e": "English", "b": "Both"}
        console.print("\n[bold]üåç –í—ã–±–µ—Ä–∏ —è–∑—ã–∫:[/bold]")
        for key, value in lang_options.items():
            console.print(f"  [cyan]{key}[/cyan] - {value}")
        
        lang_input = console.input("[bold]–í—ã–±–µ—Ä–∏ —è–∑—ã–∫ [dim](default: b)[/dim]: [/bold]")
        lang_choice = lang_input.strip().lower() if lang_input.strip() else 'b'
        
        proxy_choices = []
        for i in range(threads):
            proxy_input = console.input(f"[bold]üåê –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏ –∫ –ø—Ä–æ—Ñ–∏–ª—é {i+1}? [dim](y/n, default: n)[/dim]: [/bold]")
            use_proxy = proxy_input.strip().lower() == 'y'
            proxy_choices.append(use_proxy)
        
        return count, (delay_min, delay_max), threads, lang_choice, proxy_choices, max_tokens, temperature
        
    except (ValueError, KeyboardInterrupt):
        console.print("‚ùå [red]Invalid input or interrupted. Please try again.[/red]")
        return get_enhanced_user_inputs()

async def main():
    """Enhanced main function with memory optimization"""
    print_modern_banner()
    
    count, delay_range, threads, lang_choice, proxy_choices, max_tokens, temperature = get_enhanced_user_inputs()
    
    # Display configuration
    config_table = Table(title="‚öôÔ∏è –°—É–º–º–∞—Ä–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞", border_style="bright_cyan")
    config_table.add_column("–ù–∞—Å—Ç—Ä–æ–π–∫–∞", style="bright_magenta")
    config_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="bright_cyan")
    
    config_table.add_row("–ö–æ–ª-–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤", str(count))
    config_table.add_row("–ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏", f"{delay_range[0]:.1f}s - {delay_range[1]:.1f}s")
    config_table.add_row("üî§ –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤", str(max_tokens))
    config_table.add_row("üå°Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", str(temperature))
    config_table.add_row("–ö–æ–ª-–≤–æ –ø–æ—Ç–æ–∫–æ–≤", str(threads))
    config_table.add_row("–Ø–∑—ã–∫", {"r": "Russian", "e": "English", "b": "Both"}[lang_choice])
    config_table.add_row("–ü—Ä–æ–∫—Å–∏", str(sum(proxy_choices)))
    
    console.print("\n")
    console.print(config_table)
    
    estimated_total_tokens = count * threads * max_tokens * 2
    console.print(f"üí∞ [yellow]–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤: ~{estimated_total_tokens:,}[/yellow]")
    
    console.input("\n[bold bright_yellow]–ñ–º–µ–º Enter –¥–ª—è –Ω–∞—á–∞–ª–∞...[/bold bright_yellow]")
    
    # Load configuration files
    api_keys = load_config_file("API_keys.txt")
    if not api_keys:
        console.print("‚ùå [red]No API keys found. Please add keys to API_keys.txt[/red]")
        console.print("üí° [blue]Create API_keys.txt file and add your API keys (one per line)[/blue]")
        return
    
    system_prompts = load_prompts_for_profiles("promt.txt", threads)
    
    raw_proxies = load_config_file("proxy.txt")
    formatted_proxies = format_proxies(raw_proxies)
    proxy_pool = cycle(formatted_proxies) if formatted_proxies else cycle([None])
    
    keys_cycle = cycle(api_keys)
    
    # Initialize statistics
    stats.start_time = datetime.now()
    
    # Create progress tracking
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        MofNCompleteColumn(),
        TextColumn("‚Ä¢"),
        TimeRemainingColumn(),
        console=console,
        transient=True,  # –í–∞–∂–Ω–æ: transient=True –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    ) as progress:
        
        tasks = []
        worker_tasks = []
        
        for i in range(threads):
            proxy = next(proxy_pool) if proxy_choices[i] else None
            system_prompt = system_prompts[i]
            task_id = progress.add_task(f"Worker-{i+1}", total=count)
            tasks.append(task_id)
            
            worker_task = worker_async(
                i+1, count, delay_range, keys_cycle, proxy, 
                system_prompt, lang_choice, max_tokens, temperature, progress, task_id
            )
            worker_tasks.append(worker_task)
        
        console.print(f"\nüöÄ [bold bright_green]Starting {threads} workers with {count} requests each...[/bold bright_green]")
        console.print(f"üî§ [cyan]Token limit per response: {max_tokens} | Temperature: {temperature}[/cyan]\n")
        
        try:
            await asyncio.gather(*worker_tasks)
        except KeyboardInterrupt:
            console.print("\n‚ö†Ô∏è  [yellow]Interrupted by user[/yellow]")
        finally:
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            for task in worker_tasks:
                if not task.done():
                    task.cancel()
            
            # –û—á–∏—â–∞–µ–º —Ü–∏–∫–ª—ã
            del keys_cycle, proxy_pool
            gc.collect()
    
    # Final summary
    console.print("\n" + "="*80)
    console.print(create_stats_table())
    
    # Token usage summary
    with stats_lock:
        console.print(f"\nüí∞ [bold bright_green]–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–û–ö–ï–ù–û–í:[/bold bright_green]")
        console.print(f"üî§ –í—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: [bright_cyan]{stats.total_tokens:,}[/bright_cyan]")
        console.print(f"üì• –í—Ö–æ–¥—è—â–∏–µ —Ç–æ–∫–µ–Ω—ã: [bright_yellow]{stats.total_input_tokens:,}[/bright_yellow]")
        console.print(f"üì§ –ò—Å—Ö–æ–¥—è—â–∏–µ —Ç–æ–∫–µ–Ω—ã: [bright_green]{stats.total_output_tokens:,}[/bright_green]")
        
        if stats.successful_requests > 0:
            avg_total = stats.total_tokens / stats.successful_requests
            avg_input = stats.total_input_tokens / stats.successful_requests
            avg_output = stats.total_output_tokens / stats.successful_requests
            console.print(f"üìä –°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –∑–∞–ø—Ä–æ—Å: [bright_magenta]{avg_total:.1f}[/bright_magenta] —Ç–æ–∫–µ–Ω–æ–≤ ({avg_input:.1f} in + {avg_output:.1f} out)")
    
    console.print("\n‚úÖ [bold bright_green]Stress test completed![/bold bright_green]")
    console.print(f"üìÑ [cyan]Detailed logs saved to: ai_stress_log.txt[/cyan]")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
    gc.collect()

if __name__ == "__main__":
    try:
        if sys.version_info < (3, 7):
            console.print("‚ùå [red]Python 3.7 or higher is required[/red]")
            sys.exit(1)
            
        asyncio.run(main())
    except KeyboardInterrupt:
        console.print("\nüëã [yellow]Goodbye![/yellow]")
    except Exception as e:
        console.print(f"\nüí• [red]Unexpected error: {e}[/red]")
    finally:
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ
        gc.collect()

